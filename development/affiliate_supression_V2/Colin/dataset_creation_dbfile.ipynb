{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "#Set up, imports and installations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up service principle access to azure (used by datascience_core)\n",
    "SCOPE = \"data_science\"\n",
    "sp_client_id = dbutils.secrets.get(scope=SCOPE, key=\"sp_client_id\")\n",
    "sp_tenant_id = dbutils.secrets.get(scope=SCOPE, key=\"sp_tenant_id\")\n",
    "sp_secret_key = dbutils.secrets.get(scope=SCOPE, key=\"sp_secret_key\")\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = sp_client_id\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = sp_secret_key\n",
    "os.environ[\"AZURE_TENANT_ID\"] = sp_tenant_id\n",
    "!export AZURE_CLIENT_ID=$sp_client_id\n",
    "!export AZURE_CLIENT_SECRET=$sp_secret_key\n",
    "!export AZURE_TENANT_ID=$sp_tenant_id\n",
    "\n",
    "# service_principal_clientId=\"aa9c33c7-c449-4842-8045-52c07ebbdc97\"\n",
    "# # service_principal_secret = \"Q~H7Q~t9s.N-nPco4sRpDWMsyaQwSZtLf.Vu0\"\n",
    "# # service_principal_tenantId=\"736f9f09-0fa9-4930-86b0-bc4e9631f407\"\n",
    "\n",
    "storage_account = \"ds247dldev\"\n",
    "directory = \"raw/CRA\"\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.auth.type.\" + storage_account + \".dfs.core.windows.net\", \"OAuth\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.oauth.provider.type.\" + storage_account + \".dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    ")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.oauth2.client.id.\" + storage_account + \".dfs.core.windows.net\",\n",
    "    sp_client_id,\n",
    ")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.oauth2.client.secret.\"\n",
    "    + storage_account\n",
    "    + \".dfs.core.windows.net\",\n",
    "    sp_secret_key,\n",
    ")\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.oauth2.client.endpoint.\"\n",
    "    + storage_account\n",
    "    + \".dfs.core.windows.net\",\n",
    "    \"https://login.microsoftonline.com/\" + sp_tenant_id + \"/oauth2/token\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datascience_core.data_retrieval import ProjectDatasetManager\n",
    "import functools\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install fsspec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load in the new file and change the application id column name to match the retro file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Afilliate_supression_files = dbutils.fs.ls(\n",
    "    \"abfss://projects@\"\n",
    "    + storage_account\n",
    "    + \".dfs.core.windows.net/datascience_affiliate_suppression/20230516_DS_Export_6months.csv\"\n",
    ")\n",
    "Afilliate_supression_files = spark.read.option(\"header\", True).csv(\n",
    "    Afilliate_supression_files[0].path\n",
    ")\n",
    "Afilliate_supression_files = Afilliate_supression_files.withColumnRenamed(\n",
    "    \"LoanApplicationId\", \"ApplicationId\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load in the retro file and change the application id column name to match the new file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all_epochs = spark.read.option(\"header\", True).csv(\n",
    "    \"abfss://raw@\" + storage_account + \".dfs.core.windows.net/CRA\"\n",
    ")\n",
    "df_all_epochs = df_all_epochs.withColumnRenamed(\"App.ApplicationId\", \"ApplicationId\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# join the two files together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_complete = df_all_epochs.join(\n",
    "    Afilliate_supression_files, on=\"ApplicationId\", how=\"inner\"\n",
    ")\n",
    "df_out = df_complete.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the file to the project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "manager = ProjectDatasetManager(\"affiliate_suppression\")\n",
    "manager.register_dataset(\n",
    "    \"affiliate_suppression_May-23_6months_eda_with_retro\",\n",
    "    df_out,\n",
    "    \"affiliate_suppression training data with retro to enable scorecard and prime prediction assistance\",\n",
    "    register_as_pickle=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
